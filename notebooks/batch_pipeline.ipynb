{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0585e7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0f9b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset('7Xan7der7/us_airline_sentiment')\n",
    "df = ds['train'].to_pandas()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c06db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['airline_sentiment'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82734af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['text','airline_sentiment']]\n",
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27a8af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['airline_sentiment'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34362a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    text = re.sub(r'http+', '', text)\n",
    "    text = re.sub(r'@+', '', text)\n",
    "    text = re.sub(r'[^A-Za-z0-9]', ' ', text)\n",
    "    text = text.lower().strip()\n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "df = df.drop_duplicates(subset=['text'])\n",
    "df = df.dropna(subset=['text'])\n",
    "df.to_csv('./data/processed/batch_processed.csv', index=False)\n",
    "\n",
    "df.describe(include='all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fbeb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpaug\n",
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "aug_synonym = naw.SynonymAug(aug_src='wordnet', aug_min=1, aug_max=3)\n",
    "\n",
    "aug_insert = naw.SynonymAug(aug_src='wordnet', aug_min=1, aug_max=2, aug_p=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f5199f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def augment_text(text, augmenter, num_augmentations=1):\n",
    "    augmented_texts = []\n",
    "    for _ in range(num_augmentations):\n",
    "        try:\n",
    "            aug_text = augmenter.augment(text)\n",
    "            if aug_text and aug_text != text:\n",
    "                augmented_texts.append(aug_text)\n",
    "        except:\n",
    "            continue\n",
    "    return augmented_texts\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"AUGMENTING MINORITY CLASSES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "negative_df = df[df['airline_sentiment'] == 'negative']\n",
    "neutral_df = df[df['airline_sentiment'] == 'neutral']\n",
    "positive_df = df[df['airline_sentiment'] == 'positive']\n",
    "\n",
    "max_count = len(negative_df)\n",
    "\n",
    "neutral_needed = max_count - len(neutral_df)\n",
    "positive_needed = max_count - len(positive_df)\n",
    "\n",
    "print(f\"\\nAugmentation targets:\")\n",
    "print(f\"  • Neutral class:  {len(neutral_df):,} → {max_count:,} (need {neutral_needed:,} more)\")\n",
    "print(f\"  • Positive class: {len(positive_df):,} → {max_count:,} (need {positive_needed:,} more)\")\n",
    "\n",
    "augmented_data = []\n",
    "\n",
    "print(f\"\\nAugmenting neutral class...\")\n",
    "neutral_samples = neutral_df.sample(n=neutral_needed, replace=True, random_state=42)\n",
    "for idx, row in enumerate(neutral_samples.itertuples(), 1):\n",
    "    if idx % 500 == 0:\n",
    "        print(f\"   Processed {idx}/{neutral_needed}...\")\n",
    "    \n",
    "    augmenter = aug_synonym if idx % 2 == 0 else aug_insert\n",
    "    aug_texts = augment_text(row.text, augmenter, num_augmentations=1)\n",
    "    \n",
    "    if aug_texts:\n",
    "        augmented_data.append({\n",
    "            'airline_sentiment': row.airline_sentiment,\n",
    "            'text': f\" {row.text}\"\n",
    "        })\n",
    "\n",
    "print(f\"Neutral class augmented: {len([d for d in augmented_data if d['airline_sentiment'] == 'neutral'])} new samples\")\n",
    "\n",
    "print(f\"\\nAugmenting positive class...\")\n",
    "positive_samples = positive_df.sample(n=positive_needed, replace=True, random_state=42)\n",
    "for idx, row in enumerate(positive_samples.itertuples(), 1):\n",
    "    if idx % 500 == 0:\n",
    "        print(f\"   Processed {idx}/{positive_needed}...\")\n",
    "    \n",
    "    augmenter = aug_synonym if idx % 2 == 0 else aug_insert\n",
    "    aug_texts = augment_text(row.text, augmenter, num_augmentations=1)\n",
    "    \n",
    "    if aug_texts:\n",
    "        augmented_data.append({\n",
    "            'airline_sentiment': row.airline_sentiment,\n",
    "            'text': f\"{row.text}\"\n",
    "        })\n",
    "\n",
    "print(f\"Positive class augmented: {len([d for d in augmented_data if d['airline_sentiment'] == 'positive'])} new samples\")\n",
    "\n",
    "augmented_df = pd.DataFrame(augmented_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a70d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced = pd.concat([df, augmented_df], ignore_index=True)\n",
    "\n",
    "df = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaa964b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f5cc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "embeddings = embed_model.encode(df['text'], show_progress_bar=True, convert_to_numpy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f635bde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de36160",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import chromadb\n",
    "\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "\n",
    "collection = chroma_client.create_collection(\n",
    "    name=\"aerostream_reviews\",\n",
    "    \n",
    ")\n",
    "\n",
    "documents = df[\"text\"].astype(str).tolist()\n",
    "\n",
    "metadatas = [\n",
    "    {\n",
    "        \"review_id\": int(i),\n",
    "        \"airline_sentiment\": row[\"airline_sentiment\"]\n",
    "    }\n",
    "    for i, row in df.reset_index().iterrows()\n",
    "]\n",
    "\n",
    "ids = [str(i) for i in range(len(documents))]\n",
    "\n",
    "BATCH_SIZE = 5000\n",
    "num_batches = math.ceil(len(ids) / BATCH_SIZE)\n",
    "\n",
    "for batch_idx in range(num_batches):\n",
    "    start = batch_idx * BATCH_SIZE\n",
    "    end = start + BATCH_SIZE\n",
    "\n",
    "    collection.add(\n",
    "        ids=ids[start:end],\n",
    "        embeddings=embeddings[start:end].tolist(),\n",
    "        documents=documents[start:end],\n",
    "        metadatas=metadatas[start:end],\n",
    "    )\n",
    "\n",
    "    print(f\"Inserted batch {batch_idx + 1}/{num_batches}\")\n",
    "\n",
    "print(f\"\\nStored {len(ids)} vectors successfully in ChromaDB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7409ae86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report , f1_score\n",
    "from sklearn.metrics import confusion_matrix \n",
    "\n",
    "\n",
    "all_items = collection.get(include=['embeddings','metadatas','documents'])\n",
    "X = embeddings  \n",
    "y = df['airline_sentiment'].values\n",
    "X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(X, y, df.index, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "def evaluate_model(name , model):\n",
    "    print(f\"=== {name} ===\") \n",
    "\n",
    "    train_acc = model.score(X_train, y_train) \n",
    "    test_pred = model.predict(X_test) \n",
    "    test_acc = accuracy_score(y_test, test_pred) \n",
    "    \n",
    "    f1 = f1_score(y_test, test_pred, average=\"macro\") \n",
    "\n",
    "    print(\"Train accuracy:\", train_acc) \n",
    "    print(\"Test accuracy :\", test_acc) \n",
    "    print(\"F1-score :\", f1) \n",
    "    print(\"Gap:\", train_acc - test_acc) \n",
    "    print(classification_report(y_test, test_pred)) \n",
    "\n",
    "    print(confusion_matrix(y_test, test_pred) )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
